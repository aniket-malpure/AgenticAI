{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d9554f",
   "metadata": {},
   "source": [
    "# Data Ingestion or Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a52e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Loader\n",
    "from langchain_community.document_loaders.text import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f702c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x1cd2a1782f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader('speech.txt')\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d14fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8353ff14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 0, 'page_label': '1'}, page_content='MACHINE\\nLEARNING\\nDEEP\\nLEARNING\\nPYTHON +\\nSTATS\\nCOMPUTER VISIONNATURAL LANGUAGE PROCESSING\\nGENERATIVE AI\\nRETRIEVAL AUGUMENT GENERATION\\nVECTOR DB'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 1, 'page_label': '2'}, page_content='This course is designed for aspiring data scientists, machine learning enthusiasts, and\\nprofessionals looking to build expertise in Python programming, data analysis, machine learning,\\nand deep learning. Whether you are just starting or have some experience, this comprehensive\\ncourse will equip you with the skills needed to work with real-world datasets, apply machine\\nlearning algorithms, and deploy AI solutions. By the end of the course, you’ll have a solid\\nfoundation in AI, a portfolio of end-to-end projects, and the confidence to tackle complex\\nchallenges in data science and AI.\\nLearning Objectives\\nMaster Python Programming: Understand Python fundamentals, including data types,\\ncontrol structures, and object-oriented programming, to write efficient and reusable\\ncode.\\nHandle Data with Pandas and NumPy: Acquire skills to manipulate, clean, and\\npreprocess large datasets using Pandas and NumPy for data analysis tasks.\\nVisualize Data: Create compelling data visualizations using libraries such as Matplotlib,\\nSeaborn, and Plotly to present insights effectively.\\nUnderstand SQL & NoSQL: Gain expertise in both relational (SQL) and non-relational\\n(NoSQL) databases, including MongoDB, for storing, querying, and managing data.\\nGrasp Statistics and Probability: Understand the core concepts of statistics,\\nprobability, and hypothesis testing, applying them to data analysis and machine\\nlearning.\\nMaster Machine Learning Techniques: Learn key machine learning algorithms,\\nincluding supervised, unsupervised, and ensemble methods, and apply them to real-\\nworld problems.\\nDive into Deep Learning: Develop a strong understanding of neural networks, CNNs,\\nRNNs, and transformers, with hands-on implementation for advanced AI tasks.\\nExplore Generative AI & Vector Databases: Learn the concepts and applications of\\ngenerative models, vector databases, and retrieval-augmented generation to handle\\ncomplex AI systems.\\nBuild Real-World Projects: Implement end-to-end machine learning and AI projects,\\nfrom data preprocessing to model deployment, integrating concepts from multiple\\nmodules.\\nUltimate Data Science & GenAI Bootcamp       Page  2'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 2, 'page_label': '3'}, page_content=\"Course Information\\nNo prerequisites are required for this course. The curriculum covers everything from the\\nbasics of Python programming, statistics, and machine learning to advanced topics in deep\\nlearning, NLP, and generative AI. Whether you're a beginner or have some prior experience,\\nthe course will ensure you gain the skills needed to succeed.\\nEstimated Time\\n8 months 6hrs/week*\\nRequired Skill Level\\nBegineer\\nThe course is designed to be completed over a duration of approximately 7 to 8 months, providing\\nan in-depth exploration from Python basics to GenAI, with plenty of time for practical\\nimplementation and real-world applications.\\nPrerequisites\\nUltimate Data Science & GenAI Bootcamp       Page  3\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 3, 'page_label': '4'}, page_content='Course Instructors\\nSunny SavitaGenAI Engineer\\nLinkedin\\nKrish NaikChief AI Engineer\\nLinkedin\\nUltimate Data Science & GenAI Bootcamp       Page  4\\nSourangshu PalSenior Data Scientist\\nLinkedin\\nMonal KumarData Scientist\\nLinkedin\\nMayank AggrawalSenior ML Engineer\\nLinkedin\\nDarius B.Head of Product\\nLinkedin'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 4, 'page_label': '5'}, page_content=\"In this module, you’ll get a solid introduction to Python, covering essential programming concepts\\nsuch as variables, data types, operators, and control flow. You’ll learn how to manipulate strings,\\nlists, dictionaries, and other basic data structures. The module will also guide you through writing\\nsimple functions and using loops and conditionals effectively. By the end, you'll have a strong\\nunderstanding of Python syntax, preparing you to tackle more complex programming challenges\\nand form a foundation for learning advanced concepts.\\nUltimate Data Science & GenAI Bootcamp       Page  5\\nPython Foundations\\nModule 1\\nTopics\\nIntroduction to Python Comparison with other programming\\nlanguages, Python objects: Numbers,\\nBooleans, and Strings\\nData Structures & Operations Container objects and mutability,\\nOperators, Operator precedence and\\nassociativity\\nControl Flow Conditional statements, Loops, break\\nand continue statements\\nString Manipulation Basics of string objects, Inbuilt string\\nmethods, Splitting and joining strings,\\nString formatting functions\\nLists & Collections List methods, list comprehension, Lists as\\nstacks and queues, Tuples, sets, and\\ndictionaries, Dictionary comprehensions\\nand view objects\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 5, 'page_label': '6'}, page_content='Topics\\nFunctions & Iterators Function basics and parameter passing,\\nIterators and generator functions,\\nLambda functions, map(), reduce(),\\nfilter()\\nPython Foundations\\nModule 1\\nUltimate Data Science & GenAI Bootcamp       Page  6'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 6, 'page_label': '7'}, page_content='This module takes your Python skills further by diving into object-oriented programming (OOP)\\nconcepts like classes, inheritance, and polymorphism. You’ll also explore more advanced topics\\nsuch as decorators, lambda functions, iterators, and generator functions. Additionally, we cover\\nexception handling, file operations, and working with modules and libraries. By the end, you will be\\ncomfortable building more sophisticated Python applications and writing efficient, reusable code.\\nAdvanced Python Programming\\nModule 2\\nTopics\\nObject-Oriented Programming (OOP) OOP basics and class creation,\\nInheritance, Polymorphism,\\nEncapsulation, and Abstraction,\\nDecorators, class methods, and static\\nmethods, Special (Magic/Dunder)\\nmethods, Property decorators: Getters,\\nsetters, and delete methods\\nFile Handling & Logging Reading and writing files, Buffered read\\nand write operations, more file methods,\\nLogging and debugging\\nModules & Exception Handling Importing modules and using them\\neffectively, Exception handling\\nConcurrency & Parallelism Introduction to multithreading,\\nMultiprocessing for performance\\noptimization\\nUltimate Data Science & GenAI Bootcamp       Page  7'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 7, 'page_label': '8'}, page_content='In this module, you will master the core aspects of data manipulation using Pandas. You’ll learn\\nhow to work with Series, DataFrames, and Panels, as well as perform data selection, filtering, and\\nsorting. The module covers critical tasks like handling missing data, reindexing, and applying\\nstatistical functions to datasets. You’ll also gain hands-on experience with data visualization and\\nadvanced indexing techniques, empowering you to efficiently analyze and manipulate complex\\ndatasets.\\nMastering Data Handling with Pandas\\nTopics\\nData Structures & Fundamentals Series, DataFrame, Panel, Basic\\nFunctionality, Indexing & Selecting, Re-\\nindexing, Iteration\\nData Operations & Transformations Sorting, Working with Text Data, Options\\n& Customization, Categorical Data, Date\\nFunctionality, Time Delta\\nData Analysis & Statistical Functions Data Statistical Functions, Window\\nFunctions\\nReading, Writing & Visualization Reading Data from Different File\\nSystems, Visualization, Tools\\nUltimate Data Science & GenAI Bootcamp       Page  8\\nModule 3'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 8, 'page_label': '9'}, page_content='This module introduces you to NumPy, a key library for numerical computing in Python. You’ll learn\\nhow to create and manipulate NumPy arrays, perform advanced indexing, and understand\\nbroadcasting. The module covers essential mathematical and statistical functions, including array\\nmanipulations, binary operations, and vectorized operations. By the end, you’ll have the skills to\\nefficiently perform complex numerical computations and leverage NumPy for machine learning\\nand deep learning applications.\\nMastering NumPy\\nTopics\\nNumPy Basics & Array Creation NdArray Object, Data Types, Array\\nAttributes, Array Creation Routines,\\nArray from Existing Data, Data Array from\\nNumerical Ranges\\nIndexing, Slicing & Advanced Indexing Indexing & Slicing, Advanced Indexing\\nArray Operations & Manipulation Array Manipulation, Binary Operators,\\nString Functions, Arithmetic Operations,\\nMathematical Functions\\nMathematical & Statistical Analysis Statistical Functions, Sort, Search &\\nCounting Functions, Matrix Library,\\nLinear Algebra\\nAdvanced Concepts Broadcasting, Iterating Over Array, Byte\\nSwapping, Copies & Views\\nUltimate Data Science & GenAI Bootcamp       Page  9\\nModule 4'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 9, 'page_label': '10'}, page_content=\"In this module, you’ll learn how to visualize data effectively using Python's popular libraries,\\nMatplotlib, Seaborn, and Plotly. You’ll cover essential plot types like line charts, bar graphs, and\\nscatter plots, and learn how to customize these visualizations to highlight key insights. Additionally,\\nthe module teaches you how to visualize statistical data, correlations, and distributions, helping\\nyou communicate data-driven findings in a visually compelling way.\\nData Visualization with Python\\nTopics\\nIntroduction to Data Visualization Overview of Data Visualization, Principles\\nof Good Visualization\\nMatplotlib Introduction to Matplotlib, Creating\\nBasic Plots (Line, Bar, Scatter),\\nCustomizing Axes, Titles, Legends, and\\nLabels, Working with Subplots, Saving\\nand Exporting Figures\\nSeaborn Introduction to Seaborn, Visualizing\\nDistributions, Relationship Plots\\n(Pairplots, Heatmaps), Categorical Data\\nVisualization, Advanced Plot\\nCustomizations\\nPlotly Introduction to Plotly, Creating\\nInteractive Plots (Line, Bar, Scatter),\\nCustomizing Plots, Dashboards and\\nInteractive Layouts, Plotly Express\\nUltimate Data Science & GenAI Bootcamp       Page  10\\nModule 5\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 10, 'page_label': '11'}, page_content='This module dives into advanced SQL techniques, including complex queries, joins, and indexing\\nfor efficient data retrieval. You’ll learn how to implement stored procedures, triggers, and\\nfunctions, and explore the use of window functions and partitions. The module covers key\\ndatabase design concepts like primary and foreign keys and normalization. By the end, you’ll be\\nproficient in managing large-scale databases and optimizing SQL queries for performance.\\nAdvanced SQL and Database Management\\nTopics\\nIntroduction to SQL Introduction to SQL, SQL Queries:\\nSELECT, INSERT, UPDATE, DELETE\\nSQL Functions and Procedures SQL Functions (Aggregate, Scalar),\\nStored Procedures, User-defined\\nFunctions (UDFs), Function and\\nProcedure Syntax\\nDatabase Constraints Primary and Foreign Keys, Data Integrity,\\nReferential Integrity\\nAdvanced SQL Techniques Window Functions, Partitioning, CTE\\n(Common Table Expressions), Indexing\\nSQL Joins and Unions Inner Join, Left Join, Right Join, Full Outer\\nJoin, Cross Join, Union\\nTriggers and Case Statements Triggers (Before, After), CASE\\nStatements, Conditional Logic\\nUltimate Data Science & GenAI Bootcamp       Page  11\\nModule 6'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 11, 'page_label': '12'}, page_content='Advanced SQL and Database Management\\nTopics\\nNormalization and Pivoting Normalization Forms (1NF, 2NF, 3NF),\\nPivot Tables, Data Aggregation\\nUltimate Data Science & GenAI Bootcamp       Page  12\\nModule 6'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 12, 'page_label': '13'}, page_content=\"In this module, you'll explore the world of NoSQL databases with MongoDB. You'll learn how to\\ncreate and manage databases, collections, and documents, and perform CRUD operations. The\\nmodule covers querying, sorting, and indexing, providing a comprehensive understanding of\\nMongoDB's flexible data model. By the end, you’ll be able to efficiently work with NoSQL\\ndatabases, particularly for use cases that involve unstructured or semi-structured data.\\nIntroduction to NoSQL with MongoDB\\nTopics\\nGetting Started with MongoDB MongoDB Introduction, Setting up\\nMongoDB, MongoDB Shell Commands\\nDatabase and Collection Management MongoDB Create Database, MongoDB\\nCreate Collection\\nCRUD Operations MongoDB Insert, MongoDB Find,\\nMongoDB Update, MongoDB Delete\\nQuerying MongoDB MongoDB Query, MongoDB Sort,\\nMongoDB Limit\\nManaging Collections MongoDB Drop Collection, MongoDB\\nDelete (Specific)\\nUltimate Data Science & GenAI Bootcamp       Page  13\\nModule 7\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 13, 'page_label': '14'}, page_content='This module provides a foundation in statistics and probability, covering essential terms, concepts,\\nand methods. You’ll learn about different types of data, levels of measurement, and key statistical\\nmeasures like mean, median, variance, and standard deviation. The module introduces random\\nvariables, probability distributions, and various types of probability functions, giving you a strong\\nbase to analyze and interpret data from a statistical perspective.\\nFoundations of Statistics and Probability\\nTopics\\nIntroduction to Statistics Introduction to Basic Statistics Terms,\\nTypes of Statistics, Types of Data, Levels\\nof Measurement, Measures of Central\\nTendency, Measures of Dispersion\\nExploring Random Variables and\\nProbability\\nRandom Variables, Set Theory,\\nSkewness, Covariance and Correlation,\\nProbability Density/Distribution Function\\nDistributions and Their Applications Types of Probability Distributions,\\nBinomial Distribution, Poisson\\nDistribution, Normal Distribution\\n(Gaussian Distribution), Probability\\nDensity Function and Mass Function,\\nCumulative Density Function, Examples\\nof Normal Distribution, Bernoulli\\nDistribution, Uniform Distribution\\nStatistical Inference Z-Statistics, Central Limit Theorem,\\nEstimation, Hypothesis Testing\\nUltimate Data Science & GenAI Bootcamp       Page  14\\nModule 8'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 14, 'page_label': '15'}, page_content=\"In this module, you'll delve deeper into statistical inference techniques, including hypothesis\\ntesting, confidence intervals, and the types of errors in statistical tests. You’ll explore advanced\\nconcepts like P-values, T-tests, and Chi-square tests, learning how to interpret results in the\\ncontext of real-world data. By the end, you’ll be equipped to conduct sophisticated statistical\\nanalysis and make informed decisions based on data-driven evidence.\\nAdvanced Statistical Inference and\\nHypothesis Testing\\nTopics\\nHypothesis Testing and Errors Hypothesis Testing Mechanism, Type 1 &\\nType 2 Error, T-Tests vs. Z-Tests:\\nOverview, When to Use a T-Test vs. Z-\\nTest\\nStatistical Distributions and Tests T-Stats, Student T Distribution, Chi-\\nSquare Test, Chi-Square Distribution\\nUsing Python, Chi-Square for Goodness\\nof Fit Test\\nBayesian Statistics and Confidence\\nIntervals\\nBayes Statistics (Bayes Theorem),\\nConfidence Interval (CI), Confidence\\nIntervals and the Margin of Error,\\nInterpreting Confidence Levels and\\nConfidence Intervals\\nStatistical Significance and\\nInterpretation\\nP-Value, T-Stats vs. Z-Stats: Overview\\nUltimate Data Science & GenAI Bootcamp       Page  15\\nModule 9\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 15, 'page_label': '16'}, page_content='This module covers essential techniques for preparing and transforming data before applying\\nmachine learning models. You’ll learn how to handle missing values, deal with imbalanced data,\\nand scale or encode features. The module also explores methods for handling outliers, feature\\nselection (including forward/backward elimination), and dimensionality reduction techniques. By\\nthe end, you’ll be proficient in preparing high-quality datasets that are ready for modeling.\\nFeature Engineering and Data\\nPreprocessing\\nTopics\\nHandling Missing and Imbalanced\\nData\\nHandling Missing Data, Handling\\nImbalanced Data\\nOutliers and Scaling Handling Outliers, Feature Scaling\\nData Transformation and Encoding Data Encoding\\nFeature Selection Techniques Backward Elimination, Forward\\nElimination, Recursive Feature\\nElimination\\nCorrelation and Multicollinearity Covariance and Correlation, VIF\\nUltimate Data Science & GenAI Bootcamp       Page  16\\nModule 10'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 16, 'page_label': '17'}, page_content='In this module, you’ll learn how to perform Exploratory Data Analysis (EDA) to uncover patterns,\\ntrends, and relationships in your data. You’ll master techniques for visualizing distributions,\\nidentifying correlations, and detecting anomalies. The module emphasizes the importance of\\nsummary statistics, data cleaning, and feature engineering. By the end, you’ll be able to extract\\nmeaningful insights from raw data and prepare it for further analysis or modeling.\\nExploratory Data Analysis (EDA) for\\nDetailed Insights\\nTopics\\nTrend Analysis and Segmentation Analyzing Bike Sharing Trends,\\nCustomer Segmentation and Effective\\nCross-Selling\\nSentiment and Quality Analysis Analyzing Movie Reviews Sentiment,\\nAnalyzing Wine Types and Quality\\nRecommendation and Forecasting Analyzing Music Trends and\\nRecommendations, Forecasting Stock\\nand Commodity Prices\\nUltimate Data Science & GenAI Bootcamp       Page  17\\nModule 11'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 17, 'page_label': '18'}, page_content='This module provides a comprehensive introduction to machine learning, covering key algorithms\\nand techniques. You’ll learn the differences between supervised and unsupervised learning, as\\nwell as the core concepts of regression, classification, and clustering. The module introduces\\nmodel evaluation metrics like accuracy, precision, recall, and F1-score, giving you the foundation to\\nunderstand and implement machine learning models in real-world scenarios.\\nMachine Learning Foundations and\\nTechniques\\nTopics\\nIntroduction to Machine Learning AI vs ML vs DL vs DS, Types of ML\\nTechniques, Supervised vs Unsupervised\\nvs Semi-Supervised vs Reinforcement\\nLearning\\nLinear Regression Simple Linear Regression, Multiple Linear\\nRegression, MSE, MAE, RMSE, R-\\nsquared, Adjusted R-squared, Linear\\nRegression with OLS\\nRegularization Techniques Ridge Regression, Lasso Regression,\\nElasticNet\\nLogistic Regression Logistic Regression, Performance\\nMetrics: Confusion Matrix, Accuracy,\\nPrecision, Recall, F-Beta Score, ROC-\\nAUC Curve\\nSupport Vector Machines (SVM) Support Vector Classifiers, Support\\nVector Regressor, Support Vector\\nKernels\\nUltimate Data Science & GenAI Bootcamp       Page  18\\nModule 12'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 18, 'page_label': '19'}, page_content='Machine Learning Foundations and\\nTechniques\\nTopics\\nBayes Theorem and Naive Bayes Introduction to Bayes Theorem, Naive\\nBayes Classifier\\nK-Nearest Neighbors (KNN) KNN Classifier, KNN Regressor\\nDecision Trees Decision Tree Classifier, Decision Tree\\nRegressor\\nEnsemble Methods Bagging, Boosting, Random Forest\\nClassifier, Random Forest Regressor,\\nOut-of-Bag Evaluation, XGBoost\\nClassifier, XGBoost Regressor\\nSupport Vector Machines (SVM) Support Vector Classifiers, Support\\nVector Regressor, Support Vector\\nKernels\\nIntroduction to Unsupervised Learning Overview of Unsupervised Learning, Use\\nCases, and Applications\\nClustering Techniques KMeans Clustering, Hierarchical\\nClustering, DBSCAN Clustering\\nUltimate Data Science & GenAI Bootcamp       Page  19\\nModule 12'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 19, 'page_label': '20'}, page_content='Machine Learning Foundations and\\nTechniques\\nTopics\\nClustering Evaluation Silhouette Coefficient, Evaluation\\nMetrics for Clustering Algorithms\\nUltimate Data Science & GenAI Bootcamp       Page  20\\nModule 12'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 20, 'page_label': '21'}, page_content='In this module, you’ll explore the basics of Natural Language Processing (NLP) for machine\\nlearning applications. Topics include text preprocessing (stemming, lemmatization), tokenization,\\nand POS tagging. You’ll also learn how to implement key NLP techniques like Named Entity\\nRecognition, word embeddings (Word2Vec), and TF-IDF. By the end of this module, you’ll have the\\nskills to work with textual data and apply machine learning models to solve NLP tasks.\\nNatural Language Processing for\\nMachine Learning\\nTopics\\nIntroduction to NLP for ML Roadmap to Learn NLP for ML, Practical\\nUse Cases of NLP in Machine Learning\\nText Preprocessing Tokenization, Basic Terminology,\\nStemming, Lemmatization, Stopwords\\nText Representation One-Hot Encoding, N-Gram, Bag of\\nWords (BoW), TF-IDF Intuition\\nPart of Speech (POS) Tagging POS Tagging using NLTK, Understanding\\nPOS Tags\\nNamed Entity Recognition (NER) Introduction to NER, Implementing NER\\nwith NLTK\\nWord Embeddings Introduction to Word Embeddings,\\nBenefits of Using Word Embeddings in\\nML\\nUltimate Data Science & GenAI Bootcamp       Page  21\\nModule 13'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 21, 'page_label': '22'}, page_content='Natural Language Processing for\\nMachine Learning\\nTopics\\nWord2Vec Intuition behind Word2Vec, Training\\nWord2Vec Models, Skip-gram and\\nCBOW Architectures\\nUltimate Data Science & GenAI Bootcamp       Page  22\\nModule 13'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 22, 'page_label': '23'}, page_content='This module introduces you to deep learning and the fundamental concepts behind artificial\\nneural networks (ANNs). You’ll learn about the architecture and workings of a neural network,\\nincluding activation functions, loss functions, and optimization techniques. The module also covers\\nbackpropagation and the vanishing gradient problem. By the end, you’ll be equipped to build and\\ntrain basic neural networks and understand how deep learning models are used in AI applications.\\nIntroduction to Deep Learning and Neural\\nNetworks\\nTopics\\nIntroduction to Deep Learning Why Deep Learning Is Becoming\\nPopular?\\nPerceptron Intuition Understanding the Perceptron Model,\\nBasic Working Principle\\nArtificial Neural Network (ANN)\\nWorking\\nStructure of ANN, Neurons, Layers, and\\nHow Data Passes Through the Network\\nBackpropagation in ANN The Backpropagation Process, Gradient\\nDescent, and Training Networks\\nVanishing Gradient Problem Explanation, Causes, and Solutions\\nExploding Gradient Problem Causes and Mitigation Techniques\\nUltimate Data Science & GenAI Bootcamp       Page  23\\nModule 14'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 23, 'page_label': '24'}, page_content=\"Introduction to Deep Learning and Neural\\nNetworks\\nTopics\\nActivation Functions Different Types of Activation Functions\\n(Sigmoid, ReLU, Tanh, etc.)\\nLoss Functions Common Loss Functions for Regression\\nand Classification\\nOptimizers Types of Optimizers (SGD, Adam,\\nRMSprop, etc.)\\nWeight Initialization Techniques Methods for Initializing Weights (Xavier,\\nHe Initialization)\\nDropout Layer Concept of Dropout and its Role in\\nRegularization\\nBatch Normalization How Batch Normalization Works and\\nWhy It's Important\\nKeras Framework Fundamentals Introduction to Keras, Building Models\\nwith Keras, Basic Operations\\nPyTorch Framework Fundamentals Introduction to PyTorch, Tensor\\nOperations, Building Models with\\nPyTorch\\nUltimate Data Science & GenAI Bootcamp       Page  24\\nModule 14\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 24, 'page_label': '25'}, page_content='In this module, you’ll dive into Convolutional Neural Networks (CNNs), a cornerstone of deep\\nlearning in computer vision. You’ll learn the architecture of CNNs, including convolution layers,\\npooling layers, and fully connected layers. The module covers practical applications like image\\nclassification, object detection, and segmentation using CNNs. By the end, you’ll have hands-on\\nexperience building and training CNNs for real-world vision tasks.\\nDeep Learning : Convolutional Neural\\nNetworks (CNN) Fundamentals and\\nApplications\\nTopics\\nIntroduction to CNN CNN Fundamentals, What is\\nConvolutional Neural Network, CNN\\nArchitecture Overview\\nExplaining CNN in Detail CNN Explained in Detail, Understanding\\nTensor Space, CNN Explainer\\nCNN-Based Architectures Various CNN Architectures, Deep Dive\\ninto ResNet and its Variants\\nTraining CNN from Scratch Steps to Train CNNs, Hyperparameter\\nTuning, Overfitting, and Underfitting\\nBuilding Web Apps for CNN Deploying CNN Models into Web\\nApplications, Using Flask or Django,\\nServing Models with TensorFlow.js\\nExploding Gradient Problem Causes and Mitigation Techniques\\nUltimate Data Science & GenAI Bootcamp       Page  25\\nModule 15'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 25, 'page_label': '26'}, page_content='Deep Learning : Convolutional Neural\\nNetworks (CNN) Fundamentals and\\nApplications\\nTopics\\nObject Detection Using YOLO Introduction to YOLO (You Only Look\\nOnce), YOLO Architecture, Training and\\nDeployment\\nObject Detection Using Detectron2 Understanding Detectron2 for Object\\nDetection, Using Pre-trained Models and\\nFine-tuning\\nSegmentation Using YOLO Semantic and Instance Segmentation\\nwith YOLO, Implementing YOLO for\\nSegmentation Tasks\\nSegmentation Using Detectron2 Using Detectron2 for Semantic and\\nInstance Segmentation, Implementing\\nPre-trained Models for Image\\nSegmentation\\nUltimate Data Science & GenAI Bootcamp       Page  26\\nModule 15'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 26, 'page_label': '27'}, page_content=\"This module covers Recurrent Neural Networks (RNNs) and Transformer models, focusing on their\\napplications in sequential data processing. You’ll learn how RNNs and LSTMs are used for time\\nseries analysis, speech recognition, and language modeling. The module also explores the\\nTransformer architecture, which powers models like BERT and GPT. By the end, you'll have a\\nstrong grasp of these advanced neural network architectures and their applications in NLP and\\nbeyond.\\nDeep Learning : Recurrent Neural\\nNetworks (RNN) and Transformer\\nModels\\nTopics\\nIntroduction to RNNs Recurrent Neural Networks (RNN)\\nFundamentals, How RNNs Work,\\nApplications of RNN\\nLong Short Term Memory (LSTM) LSTM Cells, How LSTM Solves Vanishing\\nGradient Problem, LSTM for Sequence\\nModeling, Training and Tuning LSTM\\nGated Recurrent Units (GRU) GRU vs LSTM, Understanding GRU\\nArchitecture, Advantages of GRU in\\nSequence Modeling\\nEncoders and Decoders Encoder-Decoder Architecture,\\nApplications in Machine Translation,\\nSequence-to-Sequence Models\\nAttention Mechanism What is Attention, Types of Attention\\nMechanisms, Soft and Hard Attention\\nUltimate Data Science & GenAI Bootcamp       Page  27\\nModule 16\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 27, 'page_label': '28'}, page_content='Deep Learning : Recurrent Neural\\nNetworks (RNN) and Transformer\\nModels\\nTopics\\nAttention Neural Networks Self-Attention in Neural Networks,\\nApplying Attention to RNNs, Transformer\\nvs RNN\\nBERT Model BERT (Bidirectional Encoder\\nRepresentations from Transformers),\\nPre-training and Fine-tuning BERT,\\nApplications of BERT in NLP\\nGPT-2 Model GPT-2 (Generative Pre-trained\\nTransformer 2), Autoregressive\\nLanguage Modeling, Fine-tuning GPT-2\\nfor Text Generation\\nUltimate Data Science & GenAI Bootcamp       Page  28\\nModule 16'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 28, 'page_label': '29'}, page_content='In this module, you’ll explore the world of Generative AI, understanding how these models\\ngenerate new data based on patterns learned from existing data. You’ll compare generative and\\ndiscriminative models and discover their applications in text, image, and audio generation. The\\nmodule also covers advancements in generative models, including GANs and VAEs. By the end,\\nyou’ll be familiar with key concepts and applications of Generative AI.\\nIntroduction to Generative AI\\nTopics\\nOverview of Generative AI What is Generative AI?, Overview of\\nGenerative vs. Discriminative Models,\\nSignificance and Applications of\\nGenerative AI\\nUnderstanding Generative Models How Generative Models Work, Key\\nTypes of Generative Models (e.g., GANs,\\nVAEs), Advantages of Generative Models\\nGenerative AI vs. Discriminative\\nModels\\nKey Differences, Use Cases,\\nPerformance Comparison\\nRecent Advancements and Research Latest Breakthroughs in Generative AI,\\nState-of-the-Art Models and\\nTechniques, Future Trends in Generative\\nAI\\nKey Applications of Generative\\nModels\\nApplications in Art and Creativity (e.g.,\\nImage Synthesis), Healthcare (e.g., Drug\\nDiscovery), Natural Language\\nProcessing, and More\\nUltimate Data Science & GenAI Bootcamp       Page  29\\nModule 17'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 29, 'page_label': '30'}, page_content='This module introduces you to the concept of vector databases, which are designed to store and\\nretrieve high-dimensional data vectors. You’ll learn how vector databases differ from traditional\\nSQL and NoSQL databases, and explore their use cases, including similarity searches and machine\\nlearning applications. The module also covers popular vector databases like Faiss, Pinecone, and\\nChromaDB. By the end, you’ll be equipped to work with vector databases for handling complex\\ndata queries.\\nIntroduction to Vector Databases\\nTopics\\nOverview of Vector Databases What are Vector Databases?, Key\\nConcepts and Use Cases of Vector\\nDatabases, Difference Between Vector\\nDatabases and Traditional Databases\\nComparison with SQL and NoSQL\\nDatabases\\nSQL vs. NoSQL vs. Vector Databases:\\nKey Differences, Use Cases, and\\nPerformance Considerations\\nCapabilities of Vector Databases Handling High-Dimensional Data, Fast\\nSimilarity Search, Efficient Storage and\\nQuerying, Real-Time Processing\\nData Storage and Architecture of\\nVector Databases\\nStructure of Vector Data, Indexing\\nTechniques, Optimizations for Vector\\nSearch, Performance Considerations\\nTypes of Vector Databases In-Memory Vector Databases: Benefits\\nand Limitations, Local Disk-based Vector\\nDatabases, Cloud-Based Vector\\nDatabases and Their Use Cases\\nUltimate Data Science & GenAI Bootcamp       Page  30\\nModule 18'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 30, 'page_label': '31'}, page_content='Introduction to Vector Databases\\nTopics\\nExploring Popular Vector Databases Chroma DB, Faiss, Quadrant, Pinecone,\\nLanceDB: Overview, Features, and Use\\nCases\\nVector Search with NoSQL Databases Integrating Vector Search with\\nMongoDB and Cassandra, Best\\nPractices for Implementing Vector\\nSearch in NoSQL Databases\\nUltimate Data Science & GenAI Bootcamp       Page  31\\nModule 18'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 31, 'page_label': '32'}, page_content='This module introduces the concept of Retrieval-Augmented Generation (RAG), which combines\\nretrieval-based search with generative models for enhanced language generation tasks. You’ll\\nlearn about the end-to-end RAG pipeline, including how to implement it with tools like LangChain,\\nvector databases, and LLMs. The module also covers hybrid search, reranking, and multimodal\\nretrieval techniques. By the end, you’ll understand how to implement advanced RAG systems for\\nvarious use cases.\\nIntroduction to Retrieval-Augmented\\nGeneration (RAG)\\nTopics\\nOverview of Retrieval-Augmented\\nGeneration (RAG)\\nWhat is RAG?, Key Components of a\\nRAG System, Why RAG is Important for\\nAdvanced AI Systems\\nUnderstanding the End-to-End RAG\\nPipeline\\nOverview of the RAG Workflow, Data\\nRetrieval, Contextualization, and\\nGeneration Phases, Challenges and\\nOpportunities in RAG\\nIntegrating LangChain in RAG Introduction to LangChain Framework,\\nBuilding End-to-End RAG Pipelines with\\nLangChain\\nLeveraging Vector Databases in RAG Using Vector Databases for Efficient\\nRetrieval in RAG, Popular Vector\\nDatabases for RAG (e.g., Pinecone,\\nFAISS, Chroma DB)\\nRole of LLMs in RAG How LLMs (Large Language Models)\\nEnhance Generation in RAG, Fine-\\nTuning LLMs for Retrieval-Augmented\\nTasks\\nUltimate Data Science & GenAI Bootcamp       Page  32\\nModule 19'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 32, 'page_label': '33'}, page_content='Introduction to Retrieval-Augmented\\nGeneration (RAG)\\nTopics\\nRAG with Hybrid Search and\\nReranking\\nCombining Multiple Retrieval Methods,\\nReranking Results for Improved\\nRelevance, Hybrid Search\\nImplementation Techniques\\nRAG with Various Retrieval Methods Exact vs Approximate Retrieval Methods,\\nFiltering and Ranking Retrieved Data,\\nCustomizing Retrieval Approaches for\\nSpecific Applications\\nIntegrating Memory in RAG Systems How Memory Can Improve RAG,\\nPersisting and Recalling Information for\\nConsistent Results, Implementing Long-\\nTerm Memory in RAG\\nMultimodal Retrieval-Augmented\\nGeneration\\nCombining Text, Images, and Other\\nModalities in RAG, Techniques for\\nMultimodal Retrieval and Generation,\\nPractical Applications of Multimodal\\nRAG Systems\\nUltimate Data Science & GenAI Bootcamp       Page  33\\nModule 19'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': 'syllabus.pdf', 'total_pages': 34, 'page': 33, 'page_label': '34'}, page_content='In this course, you’ll gain hands-on experience in implementing end-to-end AI projects. You’ll learn\\nhow to manage the entire project lifecycle, from data collection and preprocessing to model\\ndevelopment, evaluation, and deployment. The module includes working on real-world AI projects,\\nwith a focus on best practices for integration, testing, and scalability. By the end, you’ll be\\nprepared to take on AI projects from start to finish, applying machine learning and deep learning\\ntechniques to solve real-world problems.\\nEnd-to-End AI Project Implementation\\nTopics\\nPython Project: Building End-to-End\\nApplications\\nOverview of Python Projects, Project\\nDesign and Architecture, Key\\nConsiderations in Python Projects\\n(Performance, Scalability, etc.), Best\\nPractices for Code Quality\\nEnd-to-End Machine Learning\\nProjects\\nUnderstanding End-to-End ML Projects,\\nKey Components of an End-to-End ML\\nProject, Project Example: Real-World ML\\nApplication\\nDeep Learning Projects Deep Learning Fundamentals in Projects,\\nEnd-to-End Deep Learning Projects \\nGenerative AI End-to-End Projects Introduction to Generative AI Projects,\\nSteps in Building Generative AI Projects\\nUltimate Data Science & GenAI Bootcamp       Page  34\\nPROJECT')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF Loader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('syllabus.pdf')\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7bce91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/', 'title': 'Document loaders | 🦜️🔗 LangChain', 'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nDocument loaders | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersDocument loadersCustom Document LoaderCSVFile DirectoryHTMLJSONMarkdownMicrosoft OfficePDFText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).RetrievalDocument loadersOn this pageDocument loadersinfoHead to Integrations for documentation on built-in document loader integrations with 3rd-party tools.Use document loaders to load data from a source as Document\\'s. A Document is a piece of text\\nand associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text\\ncontents of any web page, or even for loading a transcript of a YouTube video.Document loaders provide a \"load\" method for loading data as documents from a configured source. They optionally\\nimplement a \"lazy load\" as well for lazily loading data into memory.Get started\\u200bThe simplest loader reads in a file as text and places it all into one document.from langchain_community.document_loaders import TextLoaderloader = TextLoader(\"./index.md\")loader.load()API Reference:TextLoader[    Document(page_content=\\'---\\\\nsidebar_position: 0\\\\n---\\\\n# Document loaders\\\\n\\\\nUse document loaders to load data from a source as `Document`\\\\\\'s. A `Document` is a piece of text\\\\nand associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text\\\\ncontents of any web page, or even for loading a transcript of a YouTube video.\\\\n\\\\nEvery document loader exposes two methods:\\\\n1. \"Load\": load documents from the configured source\\\\n2. \"Load and split\": load documents from the configured source and split them using the passed in text splitter\\\\n\\\\nThey optionally implement:\\\\n\\\\n3. \"Lazy load\": load documents into memory lazily\\\\n\\', metadata={\\'source\\': \\'../docs/docs/modules/data_connection/document_loaders/index.md\\'})]Help us out by providing feedback on this documentation page:PreviousRetrievalNextDocument loadersGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Web based Loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader = WebBaseLoader(web_paths=(\"https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/\",),)\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8087b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/'}, page_content='infoHead to Integrations for documentation on built-in document loader integrations with 3rd-party tools.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(web_paths=(\"https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/\",),\n",
    "                       bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                           class_=(\"theme-admonition theme-admonition-info alert alert--info admonition_LlT9\")\n",
    "                       )))\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65d31848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arxiv loader\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs = ArxivLoader(query=\"1706.03762\", load_max_docs=2).load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec8901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
